{
<<<<<<< HEAD
    "tracked_experiments": 12,
=======
    "total_tracked_experiments": 20,
    "team_members": [
        "Nicolas Muhigi",
        "Ndizeye Lesly",
        "Leslie Isaro",
    ],
>>>>>>> 53ddfb5285b4cd2200075910a667cc9dfffb380f
    "experiments": [
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 1,
            "timestamp": "2025-11-16T00:54:12.865236",
            "hyperparameters": {
                "learning_rate": 1e-06,
                "gamma": 0.999,
                "batch_size": 32,
                "epsilon_start": 0.9,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very slow learning with long-term reward focus"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 2,
            "timestamp": "2025-11-16T14:36:46.402048",
            "hyperparameters": {
                "learning_rate": 0.001,
                "gamma": 0.9,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.2,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "High learning rate prioritizing short-term rewards; fast but unstable learning"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 3,
            "timestamp": "2025-11-16T15:39:55.891438",
            "hyperparameters": {
                "learning_rate": 5e-05,
                "gamma": 0.99,
                "batch_size": 8,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Tiny batch size with long exploration phase; noisy but robust learning"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 4,
            "timestamp": "2025-11-16T18:07:20.287587",
            "hyperparameters": {
                "learning_rate": 0.0002,
                "gamma": 0.995,
                "batch_size": 128,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Large batch with rapid epsilon decay; stable gradients but fast convergence"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 5,
            "timestamp": "2025-11-16T19:33:44.927399",
            "hyperparameters": {
                "learning_rate": 7e-05,
                "gamma": 0.999,
                "batch_size": 32,
                "epsilon_start": 0.5,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Semi-greedy early behavior; high gamma for long-term reward optimization"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 6,
            "timestamp": "2025-11-16T20:57:10.994488",
            "hyperparameters": {
                "learning_rate": 0.0003,
                "gamma": 0.96,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.6,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Highly exploratory agent; tests wide state-space coverage"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 7,
            "timestamp": "2025-11-16T22:13:49.401762",
            "hyperparameters": {
                "learning_rate": 2e-05,
                "gamma": 0.98,
                "batch_size": 48,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very slow epsilon decay; agent stays random for most of training"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 8,
            "timestamp": "2025-11-16T23:27:06.913159",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.005,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very greedy final policy; strong focus on exploitation performance"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 9,
            "timestamp": "2025-11-17T01:05:33.307017",
            "hyperparameters": {
                "learning_rate": 0.0008,
                "gamma": 0.92,
                "batch_size": 64,
                "epsilon_start": 0.3,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "High learning rate with low exploration; tests unstable but fast learners"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 10,
            "timestamp": "2025-11-17T15:29:52.388214",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
<<<<<<< HEAD
            "notes": "Balanced LR, stable gamma, strong exploration schedule"
=======
            "notes": "Balanced baseline setup; smooth epsilon decay with high gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 1,
            "timestamp": "2025-11-14T02:35:17.274882",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent trained successfully",
            "final_reward": null,
            "notes": "Baseline hyperparameters"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 2,
            "timestamp": "2025-11-14T03:12:45.982104",
            "hyperparameters": {
                "learning_rate": 0.00015,
                "gamma": 0.995,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent learned slightly faster",
            "final_reward": null,
            "notes": "Increased gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 3,
            "timestamp": "2025-11-14T04:05:12.654332",
            "hyperparameters": {
                "learning_rate": 0.00005,
                "gamma": 0.99,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.2
            },
            "observed_behavior": "Training stable but slower",
            "final_reward": null,
            "notes": "Lower learning rate, higher batch"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 4,
            "timestamp": "2025-11-14T04:58:27.112478",
            "hyperparameters": {
                "learning_rate": 0.0002,
                "gamma": 0.997,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent converged faster but more erratic",
            "final_reward": null,
            "notes": "Higher learning rate, lower epsilon end"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 5,
            "timestamp": "2025-11-14T05:45:53.987111",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.995,
                "batch_size": 16,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.05
            },
            "observed_behavior": "Smaller batch, less stable reward",
            "final_reward": null,
            "notes": "Small batch size"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 6,
            "timestamp": "2025-11-14T06:32:19.564832",
            "hyperparameters": {
                "learning_rate": 0.00012,
                "gamma": 0.998,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.15
            },
            "observed_behavior": "Agent explored longer, better policy",
            "final_reward": null,
            "notes": "High gamma and small epsilon end"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 7,
            "timestamp": "2025-11-14T07:21:04.234900",
            "hyperparameters": {
                "learning_rate": 0.00008,
                "gamma": 0.993,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Slower convergence, smoother reward",
            "final_reward": null,
            "notes": "Lower LR, higher batch"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 8,
            "timestamp": "2025-11-14T08:10:33.892321",
            "hyperparameters": {
                "learning_rate": 0.00018,
                "gamma": 0.996,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.03,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent faster exploration, some instability",
            "final_reward": null,
            "notes": "High LR and gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 9,
            "timestamp": "2025-11-14T09:05:47.122445",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.995,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.2
            },
            "observed_behavior": "Very low epsilon end, converged fast",
            "final_reward": null,
            "notes": "Small final epsilon"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 10,
            "timestamp": "2025-11-14T09:55:21.876553",
            "hyperparameters": {
                "learning_rate": 0.00015,
                "gamma": 0.997,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Balanced LR and gamma, stable training",
            "final_reward": null,
            "notes": "Final experiment optimized"
>>>>>>> 53ddfb5285b4cd2200075910a667cc9dfffb380f
        },
        {
  "member_name": "Leslie Isaro",
  "experiment_number": 1,
  "timestamp": "2025-11-16T09:05:11.102345",
  "hyperparameters": {
    "learning_rate": 0.00005,
    "gamma": 0.99,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.02,
    "epsilon_decay": 0.20
  },
  "observed_behavior": "Barely moved; stayed random, no learning.",
  "final_reward": null,
  "notes": "Lower learning rate + longer exploration + lower epsilon end"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 2,
  "timestamp": "2025-11-16T09:52:39.558912",
  "hyperparameters": {
    "learning_rate": 0.000075,
    "gamma": 0.995,
    "batch_size": 64,
    "epsilon_start": 1.0,
    "epsilon_end": 0.02,
    "epsilon_decay": 0.30
  },
  "observed_behavior": "Tracked slowly; limited progress with short training.",
  "final_reward": null,
  "notes": "Mid learning rate, higher gamma, bigger batch"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 3,
  "timestamp": "2025-11-16T10:34:08.734501",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.997,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.03,
    "epsilon_decay": 0.15
  },
  "observed_behavior": "Some ball tracking; brief volleys; slow but steady.",
  "final_reward": null,
  "notes": "Higher gamma with slightly longer exploration"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 4,
  "timestamp": "2025-11-16T11:16:44.219873",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.99,
    "batch_size": 16,
    "epsilon_start": 1.0,
    "epsilon_end": 0.05,
    "epsilon_decay": 0.10
  },
  "observed_behavior": "Jittery random moves; unstable and uncoordinated.",
  "final_reward": null,
  "notes": "Small batch size with faster target updates"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 5,
  "timestamp": "2025-11-16T12:01:27.906152",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.95,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.05,
    "epsilon_decay": 0.10
  },
  "observed_behavior": "Very reactive, poor control; no lasting improvement.",
  "final_reward": null,
  "notes": "Lower gamma (short-term focus)"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 6,
  "timestamp": "2025-11-16T12:47:55.481223",
  "hyperparameters": {
    "learning_rate": 0.00025,
    "gamma": 0.99,
    "batch_size": 64,
    "epsilon_start": 1.0,
    "epsilon_end": 0.05,
    "epsilon_decay": 0.30
  },
  "observed_behavior": "Faster early learning but unstable play; frequent misses.",
  "final_reward": null,
  "notes": "Higher learning rate with larger batch"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 7,
  "timestamp": "2025-11-16T13:29:21.167804",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.999,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.05,
    "epsilon_decay": 0.05
  },
  "observed_behavior": "Smoother movement; tracked ball better; multiple volleys.",
  "final_reward": null,
  "notes": "Higher gamma with short exploration"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 8,
  "timestamp": "2025-11-16T14:11:59.743118",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.99,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.01,
    "epsilon_decay": 0.10
  },
  "observed_behavior": "Plateaued early; reduced exploration froze adaptation.",
  "final_reward": null,
  "notes": "Lower learning rate run"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 9,
  "timestamp": "2025-11-16T14:58:36.315779",
  "hyperparameters": {
    "learning_rate": 0.0001,
    "gamma": 0.99,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.01,
    "epsilon_decay": 0.10
  },
  "observed_behavior": "Slight improvement; followed ball better with some over-adjustment.",
  "final_reward": null,
  "notes": "High gradient steps per update"
},
{
  "member_name": "Leslie Isaro",
  "experiment_number": 10,
  "timestamp": "2025-11-16T15:41:58.982441",
  "hyperparameters": {
    "learning_rate": 0.0002,
    "gamma": 0.97,
    "batch_size": 32,
    "epsilon_start": 1.0,
    "epsilon_end": 0.03,
    "epsilon_decay": 0.10
  },
  "observed_behavior": "Quick, consistent returns; best paddle control and rallies.",
  "final_reward": null,
  "notes": "Low gamma with fast updates â€” Best model"
}
    ]
}
