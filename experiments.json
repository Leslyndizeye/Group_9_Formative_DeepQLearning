{
<<<<<<< HEAD
    "tracked_experiments": 12,
=======
    "total_tracked_experiments": 20,
    "team_members": [
        "Nicolas Muhigi",
        "Ndizeye Lesly"
    ],
>>>>>>> 53ddfb5285b4cd2200075910a667cc9dfffb380f
    "experiments": [
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 1,
            "timestamp": "2025-11-16T00:54:12.865236",
            "hyperparameters": {
                "learning_rate": 1e-06,
                "gamma": 0.999,
                "batch_size": 32,
                "epsilon_start": 0.9,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very slow learning with long-term reward focus"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 2,
            "timestamp": "2025-11-16T14:36:46.402048",
            "hyperparameters": {
                "learning_rate": 0.001,
                "gamma": 0.9,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.2,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "High learning rate prioritizing short-term rewards; fast but unstable learning"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 3,
            "timestamp": "2025-11-16T15:39:55.891438",
            "hyperparameters": {
                "learning_rate": 5e-05,
                "gamma": 0.99,
                "batch_size": 8,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Tiny batch size with long exploration phase; noisy but robust learning"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 4,
            "timestamp": "2025-11-16T18:07:20.287587",
            "hyperparameters": {
                "learning_rate": 0.0002,
                "gamma": 0.995,
                "batch_size": 128,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Large batch with rapid epsilon decay; stable gradients but fast convergence"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 5,
            "timestamp": "2025-11-16T19:33:44.927399",
            "hyperparameters": {
                "learning_rate": 7e-05,
                "gamma": 0.999,
                "batch_size": 32,
                "epsilon_start": 0.5,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Semi-greedy early behavior; high gamma for long-term reward optimization"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 6,
            "timestamp": "2025-11-16T20:57:10.994488",
            "hyperparameters": {
                "learning_rate": 0.0003,
                "gamma": 0.96,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.6,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Highly exploratory agent; tests wide state-space coverage"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 7,
            "timestamp": "2025-11-16T22:13:49.401762",
            "hyperparameters": {
                "learning_rate": 2e-05,
                "gamma": 0.98,
                "batch_size": 48,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very slow epsilon decay; agent stays random for most of training"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 8,
            "timestamp": "2025-11-16T23:27:06.913159",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.005,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "Very greedy final policy; strong focus on exploitation performance"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 9,
            "timestamp": "2025-11-17T01:05:33.307017",
            "hyperparameters": {
                "learning_rate": 0.0008,
                "gamma": 0.92,
                "batch_size": 64,
                "epsilon_start": 0.3,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
            "notes": "High learning rate with low exploration; tests unstable but fast learners"
        },
        {
            "member_name": "Nicolas Muhigi",
            "experiment_number": 10,
            "timestamp": "2025-11-17T15:29:52.388214",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Training completed",
            "final_reward": null,
<<<<<<< HEAD
            "notes": "Balanced LR, stable gamma, strong exploration schedule"
=======
            "notes": "Balanced baseline setup; smooth epsilon decay with high gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 1,
            "timestamp": "2025-11-14T02:35:17.274882",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.99,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent trained successfully",
            "final_reward": null,
            "notes": "Baseline hyperparameters"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 2,
            "timestamp": "2025-11-14T03:12:45.982104",
            "hyperparameters": {
                "learning_rate": 0.00015,
                "gamma": 0.995,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent learned slightly faster",
            "final_reward": null,
            "notes": "Increased gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 3,
            "timestamp": "2025-11-14T04:05:12.654332",
            "hyperparameters": {
                "learning_rate": 0.00005,
                "gamma": 0.99,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.2
            },
            "observed_behavior": "Training stable but slower",
            "final_reward": null,
            "notes": "Lower learning rate, higher batch"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 4,
            "timestamp": "2025-11-14T04:58:27.112478",
            "hyperparameters": {
                "learning_rate": 0.0002,
                "gamma": 0.997,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent converged faster but more erratic",
            "final_reward": null,
            "notes": "Higher learning rate, lower epsilon end"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 5,
            "timestamp": "2025-11-14T05:45:53.987111",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.995,
                "batch_size": 16,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.05
            },
            "observed_behavior": "Smaller batch, less stable reward",
            "final_reward": null,
            "notes": "Small batch size"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 6,
            "timestamp": "2025-11-14T06:32:19.564832",
            "hyperparameters": {
                "learning_rate": 0.00012,
                "gamma": 0.998,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.02,
                "epsilon_decay": 0.15
            },
            "observed_behavior": "Agent explored longer, better policy",
            "final_reward": null,
            "notes": "High gamma and small epsilon end"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 7,
            "timestamp": "2025-11-14T07:21:04.234900",
            "hyperparameters": {
                "learning_rate": 0.00008,
                "gamma": 0.993,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Slower convergence, smoother reward",
            "final_reward": null,
            "notes": "Lower LR, higher batch"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 8,
            "timestamp": "2025-11-14T08:10:33.892321",
            "hyperparameters": {
                "learning_rate": 0.00018,
                "gamma": 0.996,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.03,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Agent faster exploration, some instability",
            "final_reward": null,
            "notes": "High LR and gamma"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 9,
            "timestamp": "2025-11-14T09:05:47.122445",
            "hyperparameters": {
                "learning_rate": 0.0001,
                "gamma": 0.995,
                "batch_size": 32,
                "epsilon_start": 1.0,
                "epsilon_end": 0.01,
                "epsilon_decay": 0.2
            },
            "observed_behavior": "Very low epsilon end, converged fast",
            "final_reward": null,
            "notes": "Small final epsilon"
        },
        {
            "member_name": "Ndizeye Lesly",
            "experiment_number": 10,
            "timestamp": "2025-11-14T09:55:21.876553",
            "hyperparameters": {
                "learning_rate": 0.00015,
                "gamma": 0.997,
                "batch_size": 64,
                "epsilon_start": 1.0,
                "epsilon_end": 0.05,
                "epsilon_decay": 0.1
            },
            "observed_behavior": "Balanced LR and gamma, stable training",
            "final_reward": null,
            "notes": "Final experiment optimized"
>>>>>>> 53ddfb5285b4cd2200075910a667cc9dfffb380f
        }
    ]
}